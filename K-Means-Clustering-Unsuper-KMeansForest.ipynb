{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#read in the data\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "#columns to drop\n",
    "#df = df.drop(['id'], axis=1)\n",
    "df.sample(frac=1)\n",
    "\n",
    "#gets rid of ? and one hot encoding for all columns that need it\n",
    "index = []\n",
    "count = 0\n",
    "for val in range(len(df.ix[:,0])):\n",
    "    flag = False\n",
    "    for column in df:\n",
    "        if df[column][val] == '?':\n",
    "            flag = True\n",
    "            break\n",
    "    if flag:\n",
    "        continue\n",
    "    if count<1000:\n",
    "        index.append(val)\n",
    "        count += 1\n",
    "df = df[df.index.isin(index)]\n",
    "\n",
    "#gets all columns which are not ints and integer encodes them\n",
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "for column in obj_df:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[column])\n",
    "    df[column] = le.transform(df[column])\n",
    "    \n",
    "#normalize all points between [0,1]\n",
    "x = df.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataset only 1100\n",
    "#create 500/500 split between labelled on nonlablled array, 1000 semi-sup data set, and 100 validation dataset\n",
    "train, test = np.split(df.sample(frac=1), [int(.6*len(df))])\n",
    "train = train.values.tolist()\n",
    "test = test.values.tolist()\n",
    "\n",
    "df_unsupervised = []\n",
    "\n",
    "label_nolabels = {}\n",
    "for point in train:\n",
    "    #unlablled 1000 points data\n",
    "    df_unsupervised.append(point[:-1])\n",
    "    label_nolabels[tuple(point[:-1])]= point[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #kmeans_forest 1-10, unsupervised learning adaboosting\n",
    "# kmeans1 = KMeans(n_clusters=2).fit(df_unsupervised)\n",
    "# # #kmeans2 = SpectralClustering(n_clusters = 2).fit_predict(df_unsupervised).tolist()\n",
    "# # kmeans3 = MeanShift().fit(df_unsupervised)\n",
    "# # #kmeans4 = AgglomerativeClustering(n_clusters=2).fit_predict(df_unsupervised).tolist()\n",
    "# # kmeans5 = DBSCAN().fit_predict(df_unsupervised).tolist()\n",
    "# # kmeans6 = GaussianMixture(n_components=2).fit(df_unsupervised)\n",
    "# # kmeans7 = Birch(n_clusters=2).fit(df_unsupervised)\n",
    "# # kmeans8 = BayesianGaussianMixture(n_components=2).fit(df_unsupervised)\n",
    "# classifiers = [kmeans1, kmeans3, kmeans5, kmeans6, kmeans7, kmeans8]\n",
    "#kmeans_forest 1-10, unsupervised learning adaboosting\n",
    "kmeans1 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans2 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans3 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans4 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans5 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans6 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans7 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans8 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans9 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised))\n",
    "kmeans10 = KMeans(n_clusters=2, init='random', n_init=10).fit(np.asarray(df_unsupervised)) \n",
    "classifiers = [kmeans1,kmeans2,kmeans3,kmeans4,kmeans5,kmeans6,kmeans7,kmeans8,kmeans9,kmeans10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv in form of rowNumber, clfNumber, clf prediction on that row\n",
    "answers = []\n",
    "for point in range(len(df_unsupervised)):\n",
    "    for clf in range(len(classifiers)):\n",
    "        answers.append([point, clf, classifiers[clf].predict([df_unsupervised[point]])])\n",
    "\n",
    "count = 0\n",
    "f = open(\"answer_file.csv\", \"w\")\n",
    "f.write('question,worker,answer;\\n')\n",
    "for answer in answers:\n",
    "    count += 1\n",
    "    f.write(str(answer[0]) + ',' + str(answer[1]) + ',' + str(int(answer[2]))+'\\n')\n",
    "f.close()\n",
    "p = open(\"result_file.csv\", \"w\")   \n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run VI BP\n",
    "import subprocess\n",
    "subprocess.call([\"python\", \"run.py\", \"methods/c_EM/method.py\", \"answer_file.csv\", \"result_file.csv\",\"decision-making\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract results, get noisy labels and \n",
    "filepath = \"result_file.csv\"\n",
    "noisy_labels = []\n",
    "with open(filepath) as fp:  \n",
    "    for line in fp:\n",
    "        questionAnswer = line.split(',')\n",
    "        noisy_labels.append(questionAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    }
   ],
   "source": [
    "#assign noisy label to proper row\n",
    "df_noise_x = [] \n",
    "df_noise_y = []\n",
    "for question in noisy_labels:\n",
    "    if question[0].rstrip() == 'question':\n",
    "        continue\n",
    "    df_noise_x += [df_unsupervised[int(question[0].rstrip())]]\n",
    "    df_noise_y.append(int(question[1].rstrip()))\n",
    "count_vi = 0\n",
    "for el in range(len(df_noise_x)):\n",
    "    if label_nolabels[tuple(df_noise_x[el])][0] != df_noise_y[el]:\n",
    "        count_vi += 1\n",
    "print(count_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noise_y2 = []\n",
    "for el in df_noise_y:\n",
    "    df_noise_y2.append(int(el))\n",
    "\n",
    "df_noise = []\n",
    "for el in range(len(df_noise_x)):\n",
    "    new = df_noise_x[el]\n",
    "    new.append(df_noise_y2[el])\n",
    "    df_noise.append(new)\n",
    "\n",
    "#need to shuffle the data\n",
    "random.shuffle(df_noise)\n",
    "\n",
    "df_noise_x = []\n",
    "df_noise_y = []\n",
    "for row in df_noise:\n",
    "    df_noise_x.append(row[:-1])\n",
    "    df_noise_y.append(row[-1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run AdaBoost from Sklearn on noisy data\n",
    "bdt2 = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "bdt2.fit(df_noise_x, df_noise_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 0.34415584415584416\n"
     ]
    }
   ],
   "source": [
    "#Ada boosting on noisy data error rate\n",
    "errors = []\n",
    "count1 = 0\n",
    "for point in test:\n",
    "    est = bdt2.predict([point[:-1]])\n",
    "    true = int(point[-1:][0])\n",
    "    est = int(est[0])\n",
    "    if est == true:\n",
    "        errors.append([point[:-1],0])\n",
    "    else:\n",
    "        count1 += 1\n",
    "        errors.append([point[:-1],1])\n",
    "\n",
    "#error rate, noisy -> baseline \n",
    "print(count1, count1/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
