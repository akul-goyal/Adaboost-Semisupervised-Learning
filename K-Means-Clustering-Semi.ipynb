{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_clf(Y_train, X_train, M, err_pos, err_neg):\n",
    "    clf = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
    "    n_train = len(X_train)\n",
    "    # Initialize weights\n",
    "    w = np.ones(n_train) / n_train\n",
    "    clf_list = []\n",
    "\n",
    "    for i in range(M):\n",
    "        # Fit a classifier with the specific weights\n",
    "        clf.fit(X_train, Y_train, sample_weight=w)\n",
    "        pred_train_i = clf.predict(X_train)\n",
    "\n",
    "        miss = []\n",
    "        # Indicator function\n",
    "        for x in range(len(pred_train_i)):\n",
    "            if pred_train_i[x]==Y_train[x]:\n",
    "                miss.append(0)\n",
    "            else:\n",
    "                miss.append(1) \n",
    "\n",
    "        # Error\n",
    "        err_m = np.dot(w, miss) / sum(w)\n",
    "        \n",
    "        # Divisor\n",
    "        dev = 1 - err_pos - err_neg\n",
    "\n",
    "        # Alpha\n",
    "        alpha_m = 0.5 * np.log((1 - err_m) / float(err_m))\n",
    "\n",
    "        #noisy correction\n",
    "        miss2 = []\n",
    "        for x in range(len(miss)):\n",
    "            if miss[x] == 0:\n",
    "               if Y_train[x] > 0:\n",
    "                  miss2.append(1 - err_neg + err_pos)\n",
    "               else:\n",
    "                   miss2.append(1 + err_neg - err_pos)\n",
    "            else:\n",
    "                if Y_train[x] > 0:\n",
    "                    miss2.append(-(1 - err_neg + err_pos))\n",
    "                else:\n",
    "                    miss2.append(-(1 + err_neg - err_pos))\n",
    "\n",
    "        w = np.multiply(w, np.exp([(float(x)/dev) * alpha_m for x in miss2]))\n",
    "        clf_list.append((clf, alpha_m))\n",
    "    return clf_list\n",
    "\n",
    "def classify_adaBoosting(clf_list, X):\n",
    "    total = 0\n",
    "    for clf in clf_list:\n",
    "        if clf[0].predict(X)>0:\n",
    "            total += clf[1]\n",
    "        else:\n",
    "            total -= clf[1]\n",
    "    if total > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#read in the data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "#columns to drop\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "df.sample(frac=1)\n",
    "#gets rid of ? and one hot encoding for all columns that need it\n",
    "index = []\n",
    "count = 0\n",
    "for val in range(len(df.ix[:,0])):\n",
    "    flag = False\n",
    "    for column in df:\n",
    "        if df[column][val] == '?':\n",
    "            flag = True\n",
    "            break\n",
    "    if flag:\n",
    "        continue\n",
    "    if count<1000:\n",
    "        index.append(val)\n",
    "        count += 1\n",
    "df = df[df.index.isin(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all columns which are not ints and integer encodes them\n",
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "for column in obj_df:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[column])\n",
    "    df[column] = le.transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize all points between [0,1]\n",
    "x = df.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = np.split(df.sample(frac=1), [int(.6*len(df))])\n",
    "unlabel, label = np.split(train.sample(frac=1), [int(.8*len(train))])\n",
    "test = test.values.tolist()\n",
    "nolabels1 = unlabel.values.tolist()\n",
    "del unlabel[0]\n",
    "nolabels = unlabel.values.tolist()\n",
    "labels = label.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = {}\n",
    "for row in nolabels1:\n",
    "    true[tuple(row[1:])] = row[:1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train f1...fn classifiers on labelled data, will use 6 types: decision stumps, knn, svm, guassian mixture mode\n",
    "#                                                               native bayes, logistic regression\n",
    "x = []\n",
    "y = []\n",
    "x_true = []\n",
    "y_true = []\n",
    "for row in labels:\n",
    "    x.append(row[1:])\n",
    "    y.append(row[:1][0])\n",
    "    x_true.append(row[1:])\n",
    "    y_true.append(row[:1][0])\n",
    "clf1 = svm.SVC(kernel='linear', gamma='scale').fit(x, y)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=3).fit(x, y)\n",
    "clf3 = DecisionTreeClassifier(splitter= 'random').fit(x, y)\n",
    "clf4 = DecisionTreeClassifier(splitter= 'random').fit(x, y)\n",
    "clf5 = DecisionTreeClassifier(splitter= 'random').fit(x, y)\n",
    "clf6 = GaussianMixture(n_components = 2, init_params= 'random').fit(x,y)\n",
    "clf7 = GaussianMixture(n_components = 2, init_params= 'random').fit(x,y)\n",
    "clf8 = GaussianMixture(n_components = 2, init_params= 'random').fit(x,y)\n",
    "clf9 = GaussianNB().fit(x,y)\n",
    "clf10 = LogisticRegression(solver='liblinear').fit(x,y)\n",
    "classifiers = [clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9, clf10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv in form of rowNumber, clfNumber, clf prediction on that row\n",
    "answers = []\n",
    "for point in range(len(nolabels)):\n",
    "    for clf in range(len(classifiers)):\n",
    "        answers.append([point, clf, classifiers[clf].predict([nolabels[point]])])\n",
    "        \n",
    "count = 0\n",
    "f = open(\"answer_file.csv\", \"w\")\n",
    "f.write('question,worker,answer;\\n')\n",
    "for answer in answers:\n",
    "    count += 1\n",
    "    f.write(str(answer[0]) + ',' + str(answer[1]) + ',' + str(int(answer[2]))+'\\n')\n",
    "f.close()\n",
    "p = open(\"result_file.csv\", \"w\")   \n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run VI BP\n",
    "import subprocess\n",
    "subprocess.call([\"python\", \"run.py\", \"methods/c_EM/method.py\", \"answer_file.csv\", \"result_file.csv\",\"decision-making\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract results, get noisy labels and \n",
    "filepath = \"result_file.csv\"\n",
    "noisy_labels = []\n",
    "with open(filepath) as fp:  \n",
    "    for line in fp:\n",
    "        questionAnswer = line.split(',')\n",
    "        noisy_labels.append(questionAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 13\n"
     ]
    }
   ],
   "source": [
    "#assign noisy label to proper row\n",
    "#combine noisy lables to real labels and randomize\n",
    "df_noise_x = [] \n",
    "df_noise_y = []\n",
    "for question in noisy_labels:\n",
    "    if question[0].rstrip() == 'question':\n",
    "        continue\n",
    "    df_noise_x += [nolabels[int(question[0].rstrip())]]\n",
    "    df_noise_y.append(int(question[1].rstrip()))\n",
    "\n",
    "\n",
    "count_vi_pos = 0\n",
    "count_vi_neg = 0\n",
    "for el in range(len(df_noise_x)):\n",
    "    if true[tuple(df_noise_x[el])] != df_noise_y[el]:\n",
    "        if df_noise_y[el] > 0:\n",
    "            count_vi_pos += 1\n",
    "        else:\n",
    "            count_vi_neg += 1\n",
    "\n",
    "#Postive and Negative Error (Hard Calculation)\n",
    "err_pos = count_vi_pos/len(df_noise_x)\n",
    "err_neg = count_vi_neg/len(df_noise_x)\n",
    "print(count_vi_pos, count_vi_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noise_x += x\n",
    "df_noise_y += y\n",
    "\n",
    "df_noise = []\n",
    "for el in range(len(df_noise_x)):\n",
    "    new = df_noise_x[el]\n",
    "    new.append(df_noise_y[el])\n",
    "    df_noise.append(new)\n",
    "\n",
    "#need to shuffle the data\n",
    "random.shuffle(df_noise)\n",
    "\n",
    "df_noise_x = []\n",
    "df_noise_y = []\n",
    "for row in df_noise:\n",
    "    df_noise_x.append(row[:-1])\n",
    "    df_noise_y.append(row[-1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in double_scalars\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/tree/tree.py:276: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.sum(sample_weight))\n"
     ]
    }
   ],
   "source": [
    "#Run Adaboosting Algo with noise correction\n",
    "clf_list_noisy = adaboost_clf(df_noise_y, df_noise_x, 20, err_pos, err_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/tree/tree.py:276: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.sum(sample_weight))\n"
     ]
    }
   ],
   "source": [
    "clf_list_clean_100 = adaboost_clf(y_true, x_true, 20, err_pos, err_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "#Ada boosting on noisy data error rate\n",
    "errors = []\n",
    "count1 = 0\n",
    "for point in test:\n",
    "    est = classify_adaBoosting(clf_list_noisy,[point[:-1]])\n",
    "    true = int(point[-1:][0])\n",
    "    if est == true:\n",
    "        errors.append([point[:-1],0])\n",
    "    else:\n",
    "        count1 += 1\n",
    "        errors.append([point[:-1],1])\n",
    "\n",
    "#Ada boosting on 500 supervised data error rate\n",
    "errors = []\n",
    "count = 0\n",
    "for point in test:\n",
    "    #est = bdt1.predict([point[:-1]])\n",
    "    est = classify_adaBoosting(clf_list_clean_100, [point[:-1]])\n",
    "    true = int(point[-1:][0])\n",
    "    #est = int(est[0])\n",
    "    if est == true:\n",
    "        errors.append([point[:-1],0])\n",
    "    else:\n",
    "        count += 1\n",
    "        errors.append([point[:-1],1])\n",
    "print(count1/len(test),count/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
